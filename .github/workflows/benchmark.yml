---
name: Benchmark Comparison

on:
  pull_request:
    branches: ["main"]
    types: [opened, synchronize, reopened]

permissions:
  contents: read
  pull-requests: write

jobs:
  benchmark:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Fetch full history for comparison

      - name: Set up Go
        uses: actions/setup-go@v5
        with:
          go-version-file: go.mod

      - name: Install benchstat
        run: go install golang.org/x/perf/cmd/benchstat@latest

      - name: Run benchmarks on current branch
        run: |
          go test -bench=. -benchmem -count=5 -timeout=5m | tee current_benchmarks.txt

      - name: Check out main branch
        run: |
          git checkout main

      - name: Set up Go with main branch Go version
        uses: actions/setup-go@v5
        with:
          go-version-file: go.mod
          cache: false

      - name: Run benchmarks on main branch
        run: |
          go test -bench=. -benchmem -count=5 -timeout=5m | tee main_benchmarks.txt || \
            echo "No benchmarks found on main branch"
          git checkout -

      - name: Compare benchmarks
        id: compare
        run: |
          if [ -f main_benchmarks.txt ] && [ -f current_benchmarks.txt ]; then
            echo "Comparing benchmarks..."
            benchstat main_benchmarks.txt current_benchmarks.txt > \
              benchmark_comparison.txt 2>&1 || true

            # Create a formatted comparison for PR comment
            cat > benchmark_report.md << 'EOF'
          ## 📊 Benchmark Comparison Results

          Comparing performance between `main` and this PR:

          ```
          EOF

            if [ -s benchmark_comparison.txt ]; then
              cat benchmark_comparison.txt >> benchmark_report.md
            else
              echo "No significant differences found or comparison failed." >> \
                benchmark_report.md
            fi

            echo '```' >> benchmark_report.md
            echo "" >> benchmark_report.md
            echo "📈 **How to interpret the results:**" >> benchmark_report.md
            echo "- **Negative values** indicate improvements" >> \
              benchmark_report.md
            echo "- **Positive values** indicate regressions" >> \
              benchmark_report.md
            echo "- **~** indicates no significant change" >> \
              benchmark_report.md
            echo "" >> benchmark_report.md
            echo "_Generated by benchmark comparison workflow_" >> \
              benchmark_report.md

            echo "has_comparison=true" >> $GITHUB_OUTPUT
          else
            echo "has_comparison=false" >> $GITHUB_OUTPUT
            echo "## 📊 Benchmark Results" > benchmark_report.md
            echo "" >> benchmark_report.md
            echo "✅ Benchmarks ran successfully on this PR:" >> \
              benchmark_report.md
            echo "" >> benchmark_report.md
            echo '```' >> benchmark_report.md
            if [ -f current_benchmarks.txt ]; then
              cat current_benchmarks.txt >> benchmark_report.md
            fi
            echo '```' >> benchmark_report.md
            echo "" >> benchmark_report.md
            echo "ℹ️ No baseline benchmarks found on main branch." >> \
              benchmark_report.md
            echo "" >> benchmark_report.md
            echo "_Generated by benchmark comparison workflow_" >> \
              benchmark_report.md
          fi

      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results
          path: |
            current_benchmarks.txt
            main_benchmarks.txt
            benchmark_comparison.txt
            benchmark_report.md
          retention-days: 30

      - name: Comment PR with benchmark results
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');

            // Read the benchmark report
            let report = '';
            try {
              report = fs.readFileSync('benchmark_report.md', 'utf8');
            } catch (error) {
              console.error('Error reading benchmark report:', error);
              return;
            }

            // Look for existing benchmark comment
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });

            const benchmarkComment = comments.find(comment =>
              comment.body.includes('📊 Benchmark') &&
              comment.user.type === 'Bot'
            );

            if (benchmarkComment) {
              // Update existing comment
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: benchmarkComment.id,
                body: report
              });
            } else {
              // Create new comment
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: report
              });
            }
